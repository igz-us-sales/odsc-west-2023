{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VERSION\"] = \"\"\n",
    "from src import create_and_set_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-29 22:35:16,465 [info] Project loaded successfully: {'project_name': 'odsc-west-2023'}\n"
     ]
    }
   ],
   "source": [
    "project = create_and_set_project(\n",
    "    name=\"odsc-west-2023\",\n",
    "    source=\"git://github.com/igz-us-sales/k3d-mlrun#master\",\n",
    "    # source=\"v3io:///bigdata/odsc-west-2023.zip\",\n",
    "    secrets_file=\"secrets.env\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-29 22:35:22,621 [warning] WARNING!, you seem to have uncommitted git changes, use .push()\n",
      "> 2023-10-29 22:35:23,277 [info] submitted pipeline odsc-west-2023-main 2023-10-29 22-35-23 id=f6ef0d0b-375d-4a6b-acce-143a2d0142e5\n",
      "> 2023-10-29 22:35:23,278 [info] Pipeline run id=f6ef0d0b-375d-4a6b-acce-143a2d0142e5, check UI for progress\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Workflow started in project odsc-west-2023 id=f6ef0d0b-375d-4a6b-acce-143a2d0142e5<div><a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/odsc-west-2023/jobs/monitor-workflows/workflow/f6ef0d0b-375d-4a6b-acce-143a2d0142e5\" target=\"_blank\">click here to view progress</a></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Pipeline running (id=f6ef0d0b-375d-4a6b-acce-143a2d0142e5), <a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/odsc-west-2023/jobs/monitor-workflows/workflow/f6ef0d0b-375d-4a6b-acce-143a2d0142e5\" target=\"_blank\"><b>click here</b></a> to view the details in MLRun UI</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.2238)\n",
       " -->\n",
       "<!-- Title: kfp Pages: 1 -->\n",
       "<svg width=\"8pt\" height=\"8pt\"\n",
       " viewBox=\"0.00 0.00 8.00 8.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 4)\">\n",
       "<title>kfp</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-4 4,-4 4,4 -4,4\"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f69594afaf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-29 22:35:23,769 [info] started run workflow odsc-west-2023-main with run id = 'f6ef0d0b-375d-4a6b-acce-143a2d0142e5' by kfp engine\n"
     ]
    }
   ],
   "source": [
    "run_id = project.run(\n",
    "    name=\"main\",\n",
    "    arguments={\n",
    "        \"dataset_name\" : \"databricks/databricks-dolly-15k\",\n",
    "        \"dataset_text_field\" : \"text\",\n",
    "        \"urls_file\" : \"./data/mlops_blogs.txt\",\n",
    "        \"pretrained_tokenizer\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"pretrained_model\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"regular_model_name\" : \"regular-adapter\",\n",
    "        \"pirate_model_name\" : \"pirate-adapter\",\n",
    "        \"max_steps\" : 1000,\n",
    "        \"logging_steps\" : 100,\n",
    "        \"save_steps\" : 500\n",
    "    },\n",
    "    dirty=True,\n",
    "    watch=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")\n",
    "serving_fn.set_env(\"CUDA_VERSION\", \"\")\n",
    "serving_fn.with_node_selection(\n",
    "    node_selector={\"app.iguazio.com/node-group\" : \"added-v100\"}\n",
    ")\n",
    "serving_fn.with_limits(gpus=1, patch=True)\n",
    "serving_fn.spec.readiness_timeout = 3000\n",
    "serving_fn.spec.min_replicas = 1\n",
    "serving_fn.spec.max_replicas = 1\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = serving_fn.set_topology(\"flow\", engine=\"async\")\n",
    "graph.to(handler=\"preprocess\", name=\"src.functions.serving.preprocess\") \\\n",
    "     .to(\"src.functions.serving.LLMModelServer\",\n",
    "         name=\"llm\",\n",
    "         model_args={\n",
    "             \"load_in_4bit\": True,\n",
    "             \"device_map\": \"auto\",\n",
    "             \"trust_remote_code\": True,\n",
    "             \"return_dict\" : True\n",
    "         },\n",
    "         tokenizer_name=model_name,\n",
    "         model_name=model_name,\n",
    "         adapters={\n",
    "             \"pirate\" : project.get_artifact_uri(\"pirate-adapter\"),\n",
    "             \"regular\" : project.get_artifact_uri(\"regular-adapter\")\n",
    "         },\n",
    "         stop_token = \"##\"\n",
    "        ).respond()\n",
    "\n",
    "serving_fn.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.deploy_function(serving_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-29 21:19:23,245 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-odsc-west-2023-serving.default-tenant.svc.cluster.local:8080/predict'}\n",
      "CPU times: user 15.5 ms, sys: 1.14 ms, total: 16.6 ms\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"can mlrun work with spark?\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\" : prompt,\n",
    "    \"adapter\" : \"pirate\",\n",
    "    \"rag\" : True,\n",
    "    \"k\" : 2,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.7,\n",
    "}\n",
    "\n",
    "response = serving_fn.invoke(path='/predict', body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye, mlrun can work with spark. Spark be a popular open-source engine for building scalable and reliable machine learning systems, and it can be used in conjunction with mlrun to manage and monitor th' spark clusters\n"
     ]
    }
   ],
   "source": [
    "print(response[\"outputs\"][\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty):\n",
    "    # Build the request for our serving graph:\n",
    "    inputs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"adapter\" : adapter.lower(),\n",
    "        \"rag\" : rag,\n",
    "        \"k\" : k,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": max_length,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "    \n",
    "    output = serving_fn.invoke(path='/predict', body=inputs)[\"outputs\"][\"prediction\"]\n",
    "    \n",
    "    if not output:\n",
    "        output = \"Context window exceeded - increase max length\"\n",
    "    \n",
    "    # Return the response:\n",
    "    return output\n",
    "\n",
    "\n",
    "# Set up a Gradio frontend application:\n",
    "with gr.Blocks(analytics_enabled=False, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"# LLM Playground\n",
    "Play with the `generate` configurations and see how they make the LLM's responses better or worse.\n",
    "\"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=5):\n",
    "            with gr.Row():\n",
    "                chatbot = gr.Chatbot()\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=\"Subject to ask about:\", placeholder=\"Type a question and Enter\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            adapter = gr.Dropdown(label=\"Adapter\", value=\"Pirate\", choices=[\"Regular\", \"Pirate\"], info=\"Choose fine tuned adapter\")\n",
    "            rag = gr.Checkbox(label=\"RAG Enrichment\", value=True)\n",
    "            k = gr.Slider(minimum=0, maximum=5, value=2, label=\"Num Sources\", step=1)\n",
    "            temperature = gr.Slider(minimum=0, maximum=1, value=0.9, label=\"Temperature\", info=\"Choose between 0 and 1\")\n",
    "            max_length = gr.Slider(minimum=0, maximum=1500, value=150, label=\"Maximum length\", info=\"Choose between 0 and 1500\")\n",
    "            top_p = gr.Slider(minimum=0, maximum=1, value=0.5, label=\"Top P\", info=\"Choose between 0 and 1\")\n",
    "            top_k = gr.Slider(minimum=0, maximum=500, value=25, label=\"Top k\", info=\"Choose between 0 and 500\")\n",
    "            repetition_penalty = gr.Slider(minimum=0, maximum=1, value=1, label=\"repetition penalty\", info=\"Choose between 0 and 1\")\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(prompt, chat_history, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty):\n",
    "        bot_message = generate(prompt, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty)\n",
    "        chat_history.append((prompt, bot_message))\n",
    "\n",
    "        return \"\", chat_history\n",
    "\n",
    "    prompt.submit(respond, [prompt, chatbot, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty], [prompt, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True, height=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-141",
   "language": "python",
   "name": "conda-env-.conda-mlrun-141-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
