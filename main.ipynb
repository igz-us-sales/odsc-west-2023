{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VERSION\"] = \"\"\n",
    "from src import create_and_set_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-30 15:14:34,473 [info] Project loaded successfully: {'project_name': 'odsc-west-2023'}\n"
     ]
    }
   ],
   "source": [
    "project = create_and_set_project(\n",
    "    name=\"odsc-west-2023\",\n",
    "    source=\"git://github.com/igz-us-sales/odsc-west-2023#master\",\n",
    "    # source=\"v3io:///bigdata/odsc-west-2023.zip\",\n",
    "    secrets_file=\"secrets.env\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-30 04:05:45,540 [warning] WARNING!, you seem to have uncommitted git changes, use .push()\n",
      "> 2023-10-30 04:05:46,916 [info] submitted pipeline odsc-west-2023-main 2023-10-30 04-05-46 id=c6547d1b-40b4-4805-9da5-56ce383e3655\n",
      "> 2023-10-30 04:05:46,917 [info] Pipeline run id=c6547d1b-40b4-4805-9da5-56ce383e3655, check UI for progress\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Workflow started in project odsc-west-2023 id=c6547d1b-40b4-4805-9da5-56ce383e3655<div><a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/odsc-west-2023/jobs/monitor-workflows/workflow/c6547d1b-40b4-4805-9da5-56ce383e3655\" target=\"_blank\">click here to view progress</a></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Pipeline running (id=c6547d1b-40b4-4805-9da5-56ce383e3655), <a href=\"https://dashboard.default-tenant.app.cst-354.iguazio-cd2.com/mlprojects/odsc-west-2023/jobs/monitor-workflows/workflow/c6547d1b-40b4-4805-9da5-56ce383e3655\" target=\"_blank\"><b>click here</b></a> to view the details in MLRun UI</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.2238)\n",
       " -->\n",
       "<!-- Title: kfp Pages: 1 -->\n",
       "<svg width=\"8pt\" height=\"8pt\"\n",
       " viewBox=\"0.00 0.00 8.00 8.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 4)\">\n",
       "<title>kfp</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-4 4,-4 4,4 -4,4\"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fa747d54760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-30 04:05:47,290 [info] started run workflow odsc-west-2023-main with run id = 'c6547d1b-40b4-4805-9da5-56ce383e3655' by kfp engine\n"
     ]
    }
   ],
   "source": [
    "run_id = project.run(\n",
    "    name=\"main\",\n",
    "    arguments={\n",
    "        \"dataset_name\" : \"databricks/databricks-dolly-15k\",\n",
    "        \"dataset_text_field\" : \"text\",\n",
    "        \"urls_file\" : \"./data/mlops_blogs.txt\",\n",
    "        \"pretrained_tokenizer\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"pretrained_model\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"regular_model_name\" : \"regular-adapter\",\n",
    "        \"pirate_model_name\" : \"pirate-adapter\",\n",
    "        \"max_steps\" : 1000,\n",
    "        \"logging_steps\" : 100,\n",
    "        \"save_steps\" : 500\n",
    "    },\n",
    "    dirty=True,\n",
    "    watch=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")\n",
    "serving_fn.set_env(\"CUDA_VERSION\", \"\")\n",
    "serving_fn.with_node_selection(\n",
    "    node_selector={\"app.iguazio.com/node-group\" : \"added-v100\"}\n",
    ")\n",
    "serving_fn.with_limits(gpus=1, patch=True)\n",
    "serving_fn.spec.readiness_timeout = 3000\n",
    "serving_fn.spec.min_replicas = 1\n",
    "serving_fn.spec.max_replicas = 1\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.2238)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"342pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 342.39 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 338.39,-40 338.39,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"41.22,-0.05 43.53,-0.15 45.81,-0.3 48.07,-0.49 50.28,-0.74 52.44,-1.03 54.54,-1.36 56.57,-1.75 58.53,-2.18 60.41,-2.65 62.19,-3.16 63.88,-3.71 65.47,-4.31 66.95,-4.94 68.32,-5.61 69.57,-6.31 70.71,-7.04 71.73,-7.8 72.62,-8.59 73.39,-9.41 74.04,-10.25 74.56,-11.11 74.97,-11.99 75.25,-12.89 75.41,-13.8 75.45,-14.72 75.38,-15.65 75.2,-16.59 74.91,-17.53 74.51,-18.47 74.02,-19.41 73.43,-20.35 72.76,-21.28 71.99,-22.2 71.15,-23.11 70.23,-24.01 69.25,-24.89 68.2,-25.75 67.09,-26.59 65.92,-27.41 64.71,-28.2 63.45,-28.96 62.14,-29.69 60.81,-30.39 59.44,-31.06 58.04,-31.69 56.62,-32.29 55.17,-32.84 53.71,-33.35 52.23,-33.82 50.73,-34.25 49.22,-34.64 47.71,-34.97 46.19,-35.26 44.65,-35.51 43.12,-35.7 41.58,-35.85 40.04,-35.95 38.5,-36 36.95,-36 35.41,-35.95 33.87,-35.85 32.33,-35.7 30.8,-35.51 29.27,-35.26 27.74,-34.97 26.23,-34.64 24.72,-34.25 23.22,-33.82 21.74,-33.35 20.28,-32.84 18.83,-32.29 17.41,-31.69 16.01,-31.06 14.64,-30.39 13.31,-29.69 12.01,-28.96 10.74,-28.2 9.53,-27.41 8.36,-26.59 7.25,-25.75 6.2,-24.89 5.22,-24.01 4.3,-23.11 3.46,-22.2 2.69,-21.28 2.02,-20.35 1.43,-19.41 0.94,-18.47 0.54,-17.53 0.25,-16.59 0.07,-15.65 0,-14.72 0.04,-13.8 0.2,-12.89 0.48,-11.99 0.89,-11.11 1.41,-10.25 2.06,-9.41 2.83,-8.59 3.72,-7.8 4.74,-7.04 5.88,-6.31 7.13,-5.61 8.5,-4.94 9.98,-4.31 11.57,-3.71 13.26,-3.16 15.04,-2.65 16.92,-2.18 18.88,-1.75 20.91,-1.36 23.01,-1.03 25.17,-0.74 27.38,-0.49 29.64,-0.3 31.92,-0.15 34.23,-0.05 36.56,0 38.89,0 41.22,-0.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"37.73\" y=\"-13.32\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- preprocess -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>preprocess</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"177.92\" cy=\"-18\" rx=\"66.47\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.92\" y=\"-13.32\" font-family=\"Times,serif\" font-size=\"14.00\">preprocess</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;preprocess -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;preprocess</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.03,-18C82.94,-18 91.58,-18 100.39,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.18,-21.5 110.18,-18 100.18,-14.5 100.18,-21.5\"/>\n",
       "</g>\n",
       "<!-- llm -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>llm</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"307.39\" cy=\"-18\" rx=\"27.01\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.39\" y=\"-13.32\" font-family=\"Times,serif\" font-size=\"14.00\">llm</text>\n",
       "</g>\n",
       "<!-- preprocess&#45;&gt;llm -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>preprocess&#45;&gt;llm</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.52,-18C252.96,-18 261.36,-18 269.12,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.97,-21.5 278.97,-18 268.97,-14.5 268.97,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fc94933a6a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = serving_fn.set_topology(\"flow\", engine=\"async\", exist_ok=True)\n",
    "graph.to(handler=\"src.functions.serving.preprocess\", name=\"preprocess\") \\\n",
    "     .to(\"src.functions.serving.LLMModelServer\",\n",
    "         name=\"llm\",\n",
    "         model_args={\n",
    "             \"load_in_4bit\": True,\n",
    "             \"device_map\": \"auto\",\n",
    "             \"trust_remote_code\": True,\n",
    "             \"return_dict\" : True\n",
    "         },\n",
    "         tokenizer_name=model_name,\n",
    "         model_name=model_name,\n",
    "         adapters={\n",
    "             \"pirate\" : project.get_artifact_uri(\"pirate-adapter\"),\n",
    "             \"regular\" : project.get_artifact_uri(\"regular-adapter\")\n",
    "         },\n",
    "         stop_token = \"##\"\n",
    "        ).respond()\n",
    "\n",
    "serving_fn.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-30 14:28:43,469 [info] Starting remote function deploy\n",
      "2023-10-30 14:28:43  (info) Deploying function\n",
      "2023-10-30 14:28:43  (info) Building\n",
      "2023-10-30 14:28:44  (info) Staging files and preparing base images\n",
      "2023-10-30 14:28:44  (info) Building processor image\n",
      "2023-10-30 14:40:45  (info) Build complete\n",
      "2023-10-30 14:52:35  (info) Function deploy complete\n",
      "> 2023-10-30 14:52:41,372 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-odsc-west-2023-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['odsc-west-2023-serving-odsc-west-2023.default-tenant.app.cst-354.iguazio-cd2.com/']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://odsc-west-2023-serving-odsc-west-2023.default-tenant.app.cst-354.iguazio-cd2.com/', 'name': 'odsc-west-2023-serving'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.deploy_function(serving_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-10-30 15:08:12,090 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-odsc-west-2023-serving.default-tenant.svc.cluster.local:8080/predict'}\n",
      "CPU times: user 16.3 ms, sys: 0 ns, total: 16.3 ms\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"can mlrun work with spark?\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\" : prompt,\n",
    "    \"adapter\" : \"pirate\",\n",
    "    \"rag\" : True,\n",
    "    \"k\" : 2,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.7,\n",
    "}\n",
    "\n",
    "response = serving_fn.invoke(path='/predict', body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye, mlrun can work with spark. Th' mlrun spark operator runtime provides a way to run spark operators within mlrun, allowing for easier management and scaling of spark applications.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"outputs\"][\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.get_function(\"serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty):\n",
    "    # Build the request for our serving graph:\n",
    "    inputs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"adapter\" : adapter.lower(),\n",
    "        \"rag\" : rag,\n",
    "        \"k\" : k,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": max_length,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "    \n",
    "    output = serving_fn.invoke(path='/predict', body=inputs)[\"outputs\"][\"prediction\"]\n",
    "    \n",
    "    if not output:\n",
    "        output = \"Context window exceeded - increase max length\"\n",
    "    \n",
    "    # Return the response:\n",
    "    return output\n",
    "\n",
    "\n",
    "# Set up a Gradio frontend application:\n",
    "with gr.Blocks(analytics_enabled=False, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"# LLM Playground\n",
    "Play with the `generate` configurations and see how they make the LLM's responses better or worse.\n",
    "\"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=5):\n",
    "            with gr.Row():\n",
    "                chatbot = gr.Chatbot()\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=\"Subject to ask about:\", placeholder=\"Type a question and Enter\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            adapter = gr.Dropdown(label=\"Adapter\", value=\"Pirate\", choices=[\"Regular\", \"Pirate\"], info=\"Choose fine tuned adapter\")\n",
    "            rag = gr.Checkbox(label=\"RAG Enrichment\", value=True)\n",
    "            k = gr.Slider(minimum=0, maximum=5, value=2, label=\"Num Sources\", step=1)\n",
    "            temperature = gr.Slider(minimum=0, maximum=1, value=0.9, label=\"Temperature\", info=\"Choose between 0 and 1\")\n",
    "            max_length = gr.Slider(minimum=0, maximum=1500, value=150, label=\"Maximum length\", info=\"Choose between 0 and 1500\")\n",
    "            top_p = gr.Slider(minimum=0, maximum=1, value=0.5, label=\"Top P\", info=\"Choose between 0 and 1\")\n",
    "            top_k = gr.Slider(minimum=0, maximum=500, value=25, label=\"Top k\", info=\"Choose between 0 and 500\")\n",
    "            repetition_penalty = gr.Slider(minimum=0, maximum=1, value=1, label=\"repetition penalty\", info=\"Choose between 0 and 1\")\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(prompt, chat_history, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty):\n",
    "        bot_message = generate(prompt, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty)\n",
    "        chat_history.append((prompt, bot_message))\n",
    "\n",
    "        return \"\", chat_history\n",
    "\n",
    "    prompt.submit(respond, [prompt, chatbot, adapter, rag, k, temperature, max_length, top_p, top_k, repetition_penalty], [prompt, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True, height=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-141",
   "language": "python",
   "name": "conda-env-.conda-mlrun-141-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
